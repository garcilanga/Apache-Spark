# Apache Spark

En esta serie vamo a trabajar con Apache Spark.

Apache Spark es un framework open source para el procesamiento de datos masivos mediante computación distribuída.

Su diseño se basa en tres pilares fundamentales: velocidad, facilidad de uso y capacidades avanzadas de analítica.

Porpociona APIs en Java, Scala, Python y R, así como un motor optimizado que soporta la ejecución de grafos en general.

También soporta un conjunto extenso y rico de herramientas de alto nivel entre las que se incluyen:
- Spark SQL, para el procesamiento de datos estructurados basado en SQL,
- MLlib, para implementar machine learning, 
- GraphX para el procesamiento de grafos y
- Spark Streaming, para procesamiento continuo (stream processing).

Spark resuelve algunsa de las limitaciones inherentes de Hadoop y MapReduce. Spark puede utilizarse junto con Hadoop, pero no es un requisito indispensable. Spark extiende el modelo MapReduce para hacerlo más rápido y habilitar más escenarios de análisis, como por ejemplo consultas interactivas y procesamiento de flujos en tiempo real. Esto es posible gracias a que Spark utiliza un cluster de cómputo en memoria (in-memory).

En esta serie de post veremos:

1. Instalar Apache Spark
2. Usar Apache Spark con Jupyter
3. ...

### Referencias

[Un Vistazo a Apache Spark Streaming](https://sg.com.mx/revista/50/un-vistazo-apache-spark-streaming)
[Apache Spark](https://es.wikipedia.org/wiki/Apache_Spark)
